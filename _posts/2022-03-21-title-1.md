---
title: "글 1"
tags: [BOS의스터디룸,bos]
mathjax: true
comments: false
published: true
---
# GNN (Fundamentals)

##### 2022.03.16 (배민우 작성)

------

### [목차]

#### [1]. permutation equivariant 와 permutaion invaraint의 차이 및 중요성

#### [2]. 'Neural Message Passing이란?' (Update, Aggregate)

#### [3]. GNN 기초 : definition of model

------

### [1]

------

기본적으로, 머신에게 이미지 데이터에 대한 학습을 시켜주는 경우

CNN (Convolutional Neural Network)를 사용한다.

그 점으로 부터, 인접행렬(adjacency matrix)의 각 성분을 이미지 파일의 픽셀로 생각하여

CNN을 적용시키려는 시도를 할 수도 있겠지만

인접행렬을 곧바로 DNN(Deep Neural Network)의 input으로 사용한다는 것은

다음과 같은 식으로서 표현하겠다는 의미이다.
$$
\begin{equation}
z_G = MLP(A[1] \oplus A[2] \oplus ... \oplus A[\lvert V \rvert])
\end{equation}
$$
이때, A[i]는 인접행렬의 각 행을 의미하며, (윗 식에서) 각 행을 vector 'concatenation' 해주고 있다.

(concatenation은 numpy concatenation함수를 떠올려 볼 때 그 의미를 잘 이해할 수 있다.)

------

위의 식과 같이 쓰기에는 분명 문제가 발생하는데, 그 이유는 ‘인접행렬의 구조 때문’이다.

즉, 인접행렬에 포함된 node의 ‘ordering’에 의존하여 DNN을 사용하게 되는 것이므로

해당 모델이 node ordering에 대해 ‘permutation equivariant’ 하지 않게 된다.

그로 인해, 우리가 인접행렬의 행 또는 열을 ‘permute’ 할 때마다 output이 바뀌게 된다.

따라서 우리가 알고자 하는 것은, ‘permutation equivariant 하게끔 neural network를

designing 하는 방법’이라고 할 수 있을 것이다.

 

이때, permutation ‘equivariant’하다는 것이 permutation ‘invariant’하다는 것과

유사한 의미로 사용될 수 있을 것 같지만, 사실 그 이름에서 이미 분명한 차이가 있다.

즉, ‘Equi-‘ 라는 접두사는 ‘같음’을 의미하고, ‘in-’은 ‘~하지 않음’을 의미하기 때문이다.

더 자세하게는 다음과 같은 수식들로 표현할 수 있겠다.
$$
Let\  f_\theta : \mathbb{X^N} \rightarrow \mathbb{Y^M} \ a \ function \ parameterized \ by\ \theta, and \ \varsigma \ be\ the\ permutation\ \\
group\ (\varsigma = S_N).\\
\cdot \ if\ M=1,\ f_\theta \ is\ permutation-invariant\ if\ f_\theta(g\cdot x)=f_\theta (x) \\
\cdot \ if\ M=N,\ f_\theta \ is\ permutation-equivariant\ if\ f_\theta(g\cdot x)=g \cdot f_\theta (x) \\
(\forall g\in\varsigma , x\in \mathbb X^N)
$$

------

### [2]

------

다음의 식을 통해서 많은 것을 한번에 설명할 수 있겠다.

우선 <img src="C:\Users\BOS의스터디룸\AppData\Roaming\Typora\typora-user-images\image-20220316151436264.png" alt="image-20220316151436264" style="zoom: 80%;" />는 node u에 대한 이웃 node들의 집합 (즉, <img src="C:\Users\BOS의스터디룸\AppData\Roaming\Typora\typora-user-images\image-20220316151516930.png" alt="image-20220316151516930" style="zoom:80%;" />)로부터 

aggregated 된 “message”를 의미한다.

보다 자세히 이해하기 위해 다음의 그림 및 설명을 살펴보자.

<img src="/home/minwoo/git/bmw970712.github.io/assets/images/GNN_aggregate.PNG" style="zoom:67%;" />

GNN (Graph Neural Network)의 각 iteration에서, AGGREGATE function은

u의 이웃 node들의 embedding의 set을 input으로 취하며, 그와 더불어

(aggregated neighborhood information에 대한) message <img src="C:\Users\BOS의스터디룸\AppData\Roaming\Typora\typora-user-images\image-20220316151436264.png" alt="image-20220316151436264" style="zoom: 80%;" />를 생성하는 함수이다.

그리고, UPDATE function은 ‘이전 단계에서의 (node u의) embedding인 <img src="C:\Users\BOS의스터디룸\AppData\Roaming\Typora\typora-user-images\image-20220316153134151.png" alt="image-20220316153134151" style="zoom:80%;" />와

message <img src="C:\Users\BOS의스터디룸\AppData\Roaming\Typora\typora-user-images\image-20220316151436264.png" alt="image-20220316151436264" style="zoom: 80%;" />를 ‘combine’ (결합)시킨다.

초기의 embeddings (즉, h의 update에 대한 식에서 k=0일 때)은

모든 node들에 대한 input features로 설정된다. (즉,<img src="C:\Users\BOS의스터디룸\AppData\Roaming\Typora\typora-user-images\image-20220316153241419.png" alt="image-20220316153241419" style="zoom:80%;" />이다.)

------

GNN message passing의 K번 iteration이 지나고 나면, ‘final layer’의 **output***을

각 node에 대한 embedding들을 정의하는 용도로 사용할 수 있게 된다.
$$
z_u = h_u^{(k)}, \forall \in V
$$
(***output** : (**참고**) 초기에 대입된 input feature가 변하는 것은 아닌 것에 주목하자.

일례로, CNN (Convolutional Neural Network)를 통해 MNIST 이미지 데이터를 다룰 때

input으로 사용되는 이미지 데이터가 변하는 것은 아니다. 다만 각 layer들을 거치면서

output들이 순차적으로 나오게 되고, 그들은 뉴런을 통해 전달된다. 마지막 layer에서는

loss function을 확인한 후 그에 따라 weight와 bias를 update해 나가는 것이다.)



이전 페이지에서 설명한 과정들을 이용하면, GNN은 permutation equivariant하도록

정의된 것이다. 그 이유는 AGGREGATE function에 대한 설명을 바탕으로 

떠올릴 수 있을 것이다. 즉, 단순히 인접행렬을 넣은 것이 아니라

‘set’을 input으로 취해준 것이기 때문이다.



(좀 전에 보았던) 그림이 설명해주고 있듯이, 각 iteration에서는 다음의 과정들이 포함된다 :

(1) 모든 node들은 그 node의 ‘local neighborhood’의 정보들을 aggregate한다.

(2) 각 node의 embedding은 graph에 ‘더 접근하면 할수록’ ‘더 많은 정보를’ 포함한다.



보다 자세히 설명하자면 아래와 같다 :

(1) k=1 에서는 (첫 iteration이 지난 직후) 모든 node embedding은 ‘1-hop’ neighborhood의 정보를 포함하게 될 것이다. 즉, 모든 node embedding이 그들의 ‘immediate graph neighbors’의 

feature들에 대한 정보를 포함하게 된다. 

(immediate graph neighbor : 길이 1만큼의 path로 접근할 수 있는 이웃)

(2) k=2 에서는 모든 node embedding이 ‘2-hop’ neighborhood의 정보를 포함한다.

왜냐하면 k=1에서는 최근접 이웃에 대한 정보를 얻은 상황이므로,

이미 ‘k=1에서 알아 둔 이웃’의 이웃에 대한 정보를 다음 iteration인 k=2에서

얻을 수 있기 때문이다. (물론 이에 대한 자세한 구현 방식은 후반 페이지에 기술하겠지만, 

다음 단계를 위해서는 이러한 개념을 이해하는 것이 중요하다.)

(k) 일반적으로 k 번의 iteration 후에는 ‘모든 node embedding이 

‘k-hop’ neighborhood에 대한 정보를 포함하게 된다.

------

그런데 과연, 이 node embedding들이 (실제로) 어떠한 방식의 '정보'를 인코딩하는가?



사실 (일반적으로) 이러한 ‘정보’는 두가지 형태로 얻어진다. 

**한가지**는 graph에 해당하는 ‘structural’한 정보이다. 예를 들어보자.

GNN message passing의 k번의 iteration이 지나고 나면, node u의

embedding <img src="file:///C:/Users/BOS의~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png" alt="img" style="zoom:80%;" />은 (대강 생각했을 때에도) u의 k-hop neighborhood에 해당하는

모든 node들 각각의 ‘degree’에 대한 정보를 인코딩할 것이다.

 

(이러한 구조적인(structural) 정보는 여러 task에서 꽤나 유용하게 쓰이는데, 

예를 들어 분자의 graph를 분석할 때, 원자의 type을 추론하는 데에 degree의 정보를

쓸 수 있고, 벤젠 고리와 같은 특수한 형태를 예측하는 데에 사용될 수 있다.)

 

**또 다른 한가지**는 ‘feature-based’ information이다.

우리가 다루는 graph에서는 모든 node들이 각각 feature를 가질 것이고,

그 정보는 GNN message passing을 initialize하는 데에 사용된다.

Message passing의 t번 iteration이 지나고 나면, 각 node의 embedding은

t-hop neighborhood의 모든 정보를 인코딩할 것이다.

 

이러한 GNN의 ‘local feature-aggregation’이 보이는 특성은, CNN에서 ‘kernel’이 

보이는 특성과 유사하다. (kernel은 filter라고도 부르는데, 그러한 필터링을 수행함으로써

Local한 정보를 따오는 부분이 유사하기 때문이다.)

하지만, CNN은 이미지에 대한 정보를 aggregate하는 것과 달리

GNN은 local한 graph neighborhoods에 대한 정보를 aggregate한다는 차이가 있다.

------

###  [3]

------

가장 기초가 되는 GNN message passing은 다음과 같이 정의된다.

<img src="file:///C:/Users/BOS의~1/AppData/Local/Temp/msohtmlclip1/01/clip_image002.png" alt="img" style="zoom:80%;" />

해당 정의에서, <img src="file:///C:/Users/BOS의~1/AppData/Local/Temp/msohtmlclip1/01/clip_image004.png" alt="img" style="zoom:80%;" /> 와 <img src="file:///C:/Users/BOS의~1/AppData/Local/Temp/msohtmlclip1/01/clip_image006.png" alt="img" style="zoom:80%;" />는 ‘trainable parameter matrices’이며,

<img src="file:///C:/Users/BOS의~1/AppData/Local/Temp/msohtmlclip1/01/clip_image008.png" alt="img" style="zoom:80%;" />는 tanh 또는 ReLU 함수와 같은 ‘non-linearity’이다.

<img src="file:///C:/Users/BOS의~1/AppData/Local/Temp/msohtmlclip1/01/clip_image010.png" alt="img" style="zoom:80%;" />는 ‘bais term’ 으로, (표기의 간편함을 위해) 종종 생략되지만 좋은 performance를

위해서는 중요하다.

 

(GNN의 message passing은, non-linearity에 의해 linear operation에 의존하는

점에서, 표준적인 MLP (Multi-Layer Perceptron)와 유사하다.)

 

이전 페이지의 정의 식을 보면, 그 의미를 (다음과 같이) 떠올려 볼 수 있다.

(i) 우선 해당 node의 이웃들로부터 받게 된 message를 합한다.

(ii) 그리고 그 정보를, 그 node의 ‘이전’ embedding을 선형결합으로 합해준다.

(iii) 최종적으로, non-linearity를 적용시켜준다.

 

이를 (앞서 배운 내용과 합하여) 정리하면 다음과 같다.

<img src="file:///C:/Users/BOS의~1/AppData/Local/Temp/msohtmlclip1/01/clip_image012.png" alt="img" style="zoom:80%;" />

<img src="file:///C:/Users/BOS의~1/AppData/Local/Temp/msohtmlclip1/01/clip_image014.png" alt="img" style="zoom:80%;" />

이와 같은 과정을 결론적으로 종합하기 위해, 다음의 식을 살펴보자.

<img src="file:///C:/Users/BOS의~1/AppData/Local/Temp/msohtmlclip1/01/clip_image016.png" alt="img" style="zoom: 67%;" />

<img src="file:///C:/Users/BOS의~1/AppData/Local/Temp/msohtmlclip1/01/clip_image018.png" alt="img" style="zoom:80%;" /> 는 GNN의 t번째 layer에서 ‘node representation matrix’이다.

(즉, 각 node는 위 행렬의 각 행에 대응한다) 그리고, **A**는 인접행렬이다.

(표기의 간편함을 위해 ‘bias term’은 생략하였다.)

물론, input graph에 self-loops를 추가해주는 것을 이용해서

Neural message passing approach를 보다 간단하게 표현할 수도 있다.

그러기 위해, 다음과 같이 표현 가능하다.

<img src="file:///C:/Users/BOS의~1/AppData/Local/Temp/msohtmlclip1/01/clip_image022.png" alt="img" style="zoom:80%;" />

위의 식에서 확인할 수 있듯이, 단위행렬 I를 인접행렬 A에 더해줌으로써

(자기 자신과 연결되었다는 의미가 추가된) self-loop의 개념을 적용시켰다.

 

이에 대해 보다 자세한 GNN approach에 대해 살펴보려고 하는데,

그전에 (아래의 수식으로) 이전에 배운 개념을 다시 확인하자.

<img src="file:///C:/Users/BOS의~1/AppData/Local/Temp/msohtmlclip1/01/clip_image024.png" alt="img" style="zoom:80%;" />

<img src="file:///C:/Users/BOS의~1/AppData/Local/Temp/msohtmlclip1/01/clip_image026.png" alt="img" style="zoom:80%;" />

이때 <img src="file:///C:/Users/BOS의~1/AppData/Local/Temp/msohtmlclip1/01/clip_image028.png" alt="img" style="zoom:80%;" />에 대해서 조정해주어야 할 부분이 있고, 그는 다음과 같은 경우에 필요하다.

: ‘해당 approach가 node degrees에 매우 민감하고 불안정할 때’

즉, (예를 들어) node u가 node u’에 비해 100배만큼 이웃이 많다고 하면

<img src="file:///C:/Users/BOS의~1/AppData/Local/Temp/msohtmlclip1/01/clip_image030.png" alt="img" style="zoom: 67%;" />

크기 정도에서 이렇게 극심한 차이가 발생하면, optimization을 할 때

적지 않은 numerical한 불안정성과 어려움을 초래하게 된다.

이를 해결하는 방법은 여러가지가 있겠으나, 우선 한가지는 단순한 ‘normalization’ 이다.

그는 곧, 합보다는 평균을 취해주는 것이다 :
$$
m_{N(u)} = \frac{\sum _{v\in N(u)}^{\ }h_v}{\left|{N\left(u\right)}\right|}
$$
하지만 Kipf와 Welling의 2016년 논문에 따르면, 다음과 같은 normalization도 적용 가능하다.
$$
m_{N(u)} = \frac{\sum _{v\in N(u)}^{\ }h_v}{\sqrt{\left|{N\left(u\right)}\right|\left|{N\left(v\right)}\right|}}
$$
이는 citation graph 처럼, 매우 높은 degree를 가진 node들이 존재할 때의 data의 경우 적절하다.

------

* Graph convolutional networks (GCNs)

  GCN은 가장 널리 쓰이는 baseline graph neural network model 중 하나로서, 

  위에서 설명한 방식의 normalization과 더불어 self-loop update approach도 함께 적용한다.

  따라서 GCN model은 다음과 같은 message passing function을 정의한다 :
  $$
  h_u^{(k)} = \sigma(W^{(k)}{\sum _{v\in N(u)\cup \left\{u\right\}}^{\ }\frac{h_v}{\sqrt{\left|{N\left(u\right)}\right|\left|{N\left(v\right)}\right|}}})
  $$

이와 같이 normalize를 해야하는가에 대한 여부는 상황마다 다르게 결정된다.

왜냐하면 normalization은 정보의 손실을 가져다줄 수 있기 때문에, 무조건 사용하는 것은 지양해야 한다.

예를 들어, normalization을 시켜준 이후에는, 서로 다른 degree를 가진 node들을 구분하기 위해

learned embedding들을 사용하기 어려워진다. 그리고 또다른 다양한 structural graph features도

모호해질 가능성이 존재한다. (따라서 normalization는 특수한 목적에 의해 사용되는 것이다.)



다시 말해서, 위와 같은 식에서의 normalization을 다시 보면, node의 degree로 나누어주고 있기 때문에

structural information이 손실되는 것이다.

그래서 보통은 node feature information이 structural information보다 훨씬 더 유용할 때 이거나

node degree의 범위가 너무 넓어서 optimization을 수행하는 동안 불안정성을 초래할 위험이 있을 경우에

이러한 normalization을 이용한다.

------
